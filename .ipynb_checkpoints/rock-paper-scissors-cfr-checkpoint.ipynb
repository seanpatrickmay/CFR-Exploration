{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving Rock-Paper-Scissors using CFR\n",
    "\n",
    "Paper: http://modelai.gettysburg.edu/2013/cfr/cfr.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = 3 # 3 Actions, R, P, S\n",
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick mixed action according to probability given\n",
    "def get_action(strategy):\n",
    "    return np.random.choice(np.arange(NUM_ACTIONS), p=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rock-Paper-Scissors Player class\n",
    "class RPSPlayer:\n",
    "    # Can initialize with given strategy\n",
    "    def __init__(self, strategy_sum=np.zeros(NUM_ACTIONS)):\n",
    "        self.regret_sum = np.zeros(NUM_ACTIONS) # Cumulative regret table, 0, 0, 0 for R, P, S\n",
    "        self.strategy_sum = strategy_sum # Cumulative strategy table\n",
    "\n",
    "    # Trains given number of iterations, by calculating utility, updating sum values.\n",
    "    def self_train(self, opponent_strategy, iterations=10000):\n",
    "        action_utility = np.zeros(NUM_ACTIONS)\n",
    "        for _ in range(iterations):\n",
    "            strategy = self.get_strategy()\n",
    "            self_action = get_action(strategy)\n",
    "            opponent_action = get_action(opponent_strategy)\n",
    "            \n",
    "            action_utility[opponent_action] = 0;\n",
    "            action_utility[(opponent_action + 1) % 3] = 1\n",
    "            action_utility[(opponent_action - 1) % 3] = -1\n",
    "\n",
    "            for action in range(NUM_ACTIONS):\n",
    "                self.regret_sum[action] += action_utility[action] - action_utility[self_action]\n",
    "\n",
    "    # Gets strategy based on regret sum table\n",
    "    def get_strategy(self):\n",
    "        strategy = np.zeros(NUM_ACTIONS) # Strategy table\n",
    "        normalizing_sum = 0\n",
    "        for action in range(NUM_ACTIONS):\n",
    "            strategy[action] = self.regret_sum[action] if (self.regret_sum[action] > 0) else 0\n",
    "            normalizing_sum += strategy[action]\n",
    "        for action in range(NUM_ACTIONS):\n",
    "            if normalizing_sum > 0:\n",
    "                strategy[action] /= normalizing_sum\n",
    "            else:\n",
    "                strategy[action] = 1.0 / NUM_ACTIONS\n",
    "            self.strategy_sum[action] += strategy[action]\n",
    "        return strategy\n",
    "\n",
    "    # Total average strategy, to be used after minimizing regret over many iterations.\n",
    "    # Uses strategy_sum to normalize, instead of regret_sum\n",
    "    def get_average_strategy(self):\n",
    "        average_strategy = np.zeros(NUM_ACTIONS)\n",
    "        normalizing_sum = sum(self.strategy_sum)\n",
    "        for action in range(NUM_ACTIONS):\n",
    "            if normalizing_sum > 0:\n",
    "                average_strategy[action] = self.strategy_sum[action] / normalizing_sum\n",
    "            else:\n",
    "                average_strategy[action] = 1.0 / NUM_ACTIONS\n",
    "        return average_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to print strategy nicely\n",
    "def format_strategy(strategy):\n",
    "    return \"Rock %: \" + str(strategy[0]) + \"\\nPaper %: \" + str(strategy[1]) + \"\\nScissors %: \" + str(strategy[2]) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial strategies:\n",
      "\n",
      "Player 1:\n",
      " Rock %: 0.6393915768472631\n",
      "Paper %: 0.041171115761381764\n",
      "Scissors %: 0.31943730739135523\n",
      "\n",
      "Player 2:\n",
      " Rock %: 0.5643345431415068\n",
      "Paper %: 0.13211443878454807\n",
      "Scissors %: 0.3035510180739453\n",
      "\n",
      "----- After 0 iterations: ----- \n",
      "\n",
      "Player 1 Strategy:\n",
      " Rock %: 0.43535274783797656\n",
      "Paper %: 0.23594592747601614\n",
      "Scissors %: 0.32870132468600727\n",
      "\n",
      "Player 2 Strategy:\n",
      " Rock %: 0.41033373660272443\n",
      "Paper %: 0.2662603684837382\n",
      "Scissors %: 0.3234058949135373\n",
      "\n",
      "----- After 10 iterations: ----- \n",
      "\n",
      "Player 1 Strategy:\n",
      " Rock %: 0.20171267725422887\n",
      "Paper %: 0.20468859923600213\n",
      "Scissors %: 0.593598723509769\n",
      "\n",
      "Player 2 Strategy:\n",
      " Rock %: 0.39844932796267424\n",
      "Paper %: 0.3390774393674441\n",
      "Scissors %: 0.26247323266988165\n",
      "\n",
      "----- After 100 iterations: ----- \n",
      "\n",
      "Player 1 Strategy:\n",
      " Rock %: 0.37207844356837316\n",
      "Paper %: 0.2746400072734566\n",
      "Scissors %: 0.35328154915817017\n",
      "\n",
      "Player 2 Strategy:\n",
      " Rock %: 0.2464188840579309\n",
      "Paper %: 0.40055576629345413\n",
      "Scissors %: 0.35302534964861504\n",
      "\n",
      "----- After 1000 iterations: ----- \n",
      "\n",
      "Player 1 Strategy:\n",
      " Rock %: 0.31483347442350884\n",
      "Paper %: 0.3469667995194064\n",
      "Scissors %: 0.33819972605708476\n",
      "\n",
      "Player 2 Strategy:\n",
      " Rock %: 0.3537046664768795\n",
      "Paper %: 0.28644479284530033\n",
      "Scissors %: 0.3598505406778202\n",
      "\n",
      "----- After 10000 iterations: ----- \n",
      "\n",
      "Player 1 Strategy:\n",
      " Rock %: 0.34410567346479753\n",
      "Paper %: 0.320074091915961\n",
      "Scissors %: 0.33582023461924154\n",
      "\n",
      "Player 2 Strategy:\n",
      " Rock %: 0.32779336886986454\n",
      "Paper %: 0.33126342194916797\n",
      "Scissors %: 0.3409432091809675\n",
      "\n",
      "----- After 100000 iterations: ----- \n",
      "\n",
      "Player 1 Strategy:\n",
      " Rock %: 0.3346247958005811\n",
      "Paper %: 0.3316101735247855\n",
      "Scissors %: 0.3337650306746335\n",
      "\n",
      "Player 2 Strategy:\n",
      " Rock %: 0.33256447047940574\n",
      "Paper %: 0.3353195353413883\n",
      "Scissors %: 0.332115994179206\n",
      "\n",
      "----- After 500000 iterations: ----- \n",
      "\n",
      "Player 1 Strategy:\n",
      " Rock %: 0.33318338965336375\n",
      "Paper %: 0.33274384983935995\n",
      "Scissors %: 0.3340727605072763\n",
      "\n",
      "Player 2 Strategy:\n",
      " Rock %: 0.3343086842571057\n",
      "Paper %: 0.33365959400999434\n",
      "Scissors %: 0.3320317217329\n",
      "\n",
      "----- After 1000000 iterations: ----- \n",
      "\n",
      "Player 1 Strategy:\n",
      " Rock %: 0.33257688913190536\n",
      "Paper %: 0.3332747345600777\n",
      "Scissors %: 0.33414837630801697\n",
      "\n",
      "Player 2 Strategy:\n",
      " Rock %: 0.33377835209458695\n",
      "Paper %: 0.3332515897580396\n",
      "Scissors %: 0.33297005814737346\n",
      "\n",
      "----- After 2000000 iterations: ----- \n",
      "\n",
      "Player 1 Strategy:\n",
      " Rock %: 0.3338016258569071\n",
      "Paper %: 0.33243529679473804\n",
      "Scissors %: 0.33376307734835475\n",
      "\n",
      "Player 2 Strategy:\n",
      " Rock %: 0.3330928659437486\n",
      "Paper %: 0.33298255256190445\n",
      "Scissors %: 0.33392458149434695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise: RPS Equilibrium\n",
    "\n",
    "unbalanced_strategy = np.array([3, 4, 5])\n",
    "random_strategy = np.random.dirichlet(alpha=[1, 1, 1]) # Method to get random strategy (sums to 1)\n",
    "random_strategy2 = np.random.dirichlet(alpha=[1, 1, 1])\n",
    "player1 = RPSPlayer(random_strategy)\n",
    "player2 = RPSPlayer(random_strategy2)\n",
    "\n",
    "# Epoch #'s to print current strategies, to see as they develop\n",
    "data_points = (0, 10, 100, 1000, 10000, 100000, 500000, 1000000, 2000000)\n",
    "\n",
    "# View the initial strategies, before regret min\n",
    "print(\"Initial strategies:\\n\")\n",
    "print(\"Player 1:\\n\", format_strategy(player1.get_average_strategy()))\n",
    "print(\"Player 2:\\n\", format_strategy(player2.get_average_strategy()))\n",
    "\n",
    "for epoch in range(2000001):\n",
    "    # Set iterations to one in self training, so players can update after each.\n",
    "    # This minimizes magnitude of walk away from equilibrium, in this case (0.3-, 0.3-, 0.3-)\n",
    "    player2_strategy = player2.get_strategy()\n",
    "    player1.self_train(player2_strategy, iterations=1)\n",
    "    player1_strategy = player1.get_strategy()\n",
    "    player2.self_train(player1_strategy, iterations=1)\n",
    "    if epoch in data_points:\n",
    "        print(f\"----- After {epoch} iterations: ----- \\n\")\n",
    "        print(\"Player 1 Strategy:\\n\", format_strategy(player1.get_average_strategy()))\n",
    "        print(\"Player 2 Strategy:\\n\", format_strategy(player2.get_average_strategy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next:\n",
    "# Exercise: Colonel Blotto"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
